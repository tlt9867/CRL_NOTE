{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# introduction to CRL\n",
    "## part1\n",
    "  - RL -> CRL\n",
    "![This is an image](./screenshot1.png)\n",
    "  - (structural causal model)   SCM \n",
    "  ![This is an image](./scs2.png)\n",
    "  - inverse probability weighting (IPW)\n",
    "\n",
    "# Causal inference\n",
    "- if Y is a child of X, then X is the direct cause of Y\n",
    "- if Y is the descendent of X, the X is a potential cause of Y\n",
    "\n",
    "# Sarsa(on-policy)\n",
    "- Derive TD target\n",
    "  - $Q_\\pi(s_t,a_t) = \\mathbb E{[R_t+\\gamma\\cdot Q_\\pi(S_{t+1},A_{t+1})]} \\approx y_t$, for all $\\pi$\n",
    "  - $\\approx r_t+\\gamma\\cdot Q_\\pi(s_{t+1},a_{t+1})$\n",
    "  - encourage $Q_\\pi(s_t,a_t)$ to approximate $y_t$\n",
    "- Tabular version\n",
    "  - observe a transition $(s_t,a_t,r_t,s_{t+1})$\n",
    "  - sample $a_{t+1} \\sim \\pi(\\cdot|s_{t+1})$, where $\\pi$ is the policy function\n",
    "  - TD target: $y_t=r_t+\\gamma\\cdot Q_\\pi(s_{t+1},a_{t+1})$\n",
    "  - TD error: $\\delta_t = Q_\\pi(s_t,a_t )-y_t$\n",
    "  - Update: $Q_\\pi(s_t,a_t) \\leftarrow Q_\\pi(s_t,a_t)-\\alpha\\cdot \\delta_t$\n",
    "- Value network version\n",
    "  - Approximate $Q_\\pi(s_t,a_t)$ by the value network, $q(s,a;\\omega)$\n",
    "  - learn $\\omega$\n",
    "- TD error and Gradient\n",
    "  - TD target: $y_t = r_t +\\gamma\\cdot q(s_{t+1},a_{t+1};\\omega)$\n",
    "  - TD error: $\\delta_t = q(s_t,a_t;\\omega)-y_t$\n",
    "  - Loss: $\\delta^2_t/2$\n",
    "  - Gradient: $\\frac{\\partial \\delta^2_t/2}{\\partial \\omega}=\\delta_t\\cdot \\frac{\\partial q(s_t,a_t;\\omega)}{\\partial \\omega}$\n",
    "  - Gradient descent: $\\omega \\leftarrow \\omega-\\alpha\\cdot \\delta_t\\cdot \\frac{\\partial q(s_t,a_t;\\omega)}{\\partial \\omega}$\n",
    "# Q-learning(off-policy)\n",
    "- Sarsa VS Q-learning\n",
    "  - Sarsa is for training action-value function, $Q_\\pi(s,a)$\n",
    "  - TD target: $y_t = r_t +\\gamma\\cdot q(s_{t+1},a_{t+1};\\omega)$\n",
    "  - Used Sarsa for updating value network(critic)\n",
    "  - Q-learning is for training the optimal action-value function, $Q^\\ast(s,a)$\n",
    "  - TD target: $y_t = r_t +\\gamma\\cdot \\max\\limits_a Q^\\ast(s_{t+1},a)$\n",
    "  - Used Q-learning for updating DQN\n",
    "- Derive TD target\n",
    "  - $Q_\\pi(s_t,a_t)= \\mathbb E[R_t+\\gamma\\cdot Q_\\pi(S_{t+1},A_{t+1})]$ for all $\\pi$\n",
    "  - if $\\pi$ is the optimal policy $\\pi^\\ast$, then <br> &emsp;$Q_{\\pi^\\ast}(s_t,a_t)= \\mathbb E[R_t+\\gamma\\cdot Q_{\\pi^\\ast}(S_{t+1},A_{t+1})]$ \n",
    "  - $Q_{\\pi^\\ast}$ and $Q^\\ast$ both denote the optimal action-value function\n",
    "  - identity: $Q^\\ast(s_t,a_t)= \\mathbb E[R_t+\\gamma\\cdot Q^\\ast(S_{t+1},A_{t+1})]$ \n",
    "    - The action $A_{t+1} = \\argmax\\limits_aQ^\\ast(S_{t+1},a)$\n",
    "    - $Q^\\ast(s_t,a_t)= \\mathbb E[R_t+\\gamma\\cdot \\max\\limits_aQ^\\ast(S_{t+1},a)]$\n",
    "    - $\\approx r_t+\\gamma\\cdot \\max\\limits_aQ^\\ast(s_{t+1},a)$ which is the TD target $y_t$\n",
    "  - Tabular version\n",
    "    - observe a transition $(s_t,a_t,r_t,s_{t+1})$\n",
    "    - TD target: $y_t =r_t+\\gamma\\cdot \\max\\limits_aQ^\\ast(s_{t+1},a)$\n",
    "    - TD error: $\\delta_t = Q^\\ast(s_t,a_t)-y_t$\n",
    "    - Update: $Q^\\ast(s_t,a_t)\\leftarrow Q^\\ast(s_t,a_t)-\\alpha\\cdot \\delta_t$\n",
    "  - DQN version\n",
    "    - Approximate $Q^\\ast(s_t,a_t)$ by DQN, $Q(s,a;\\omega)$\n",
    "    - DQN controls the agent by:$a_t=\\argmax\\limits_aQ(s_t,a;\\omega)$\n",
    "    - learn $\\omega$\n",
    "    - Observe a transition $(s_t,a_t,r_t,s_{t+1})$\n",
    "    - TD target: $y_t =r_t+\\gamma\\cdot \\max\\limits_aQ(s_{t+1},a;\\omega)$\n",
    "    - TD error: $\\delta_t = Q(s_t,a_t;\\omega)-y_t$\n",
    "    - Update: $\\omega\\leftarrow \\omega-\\alpha\\cdot \\delta_t\\cdot \\frac{\\partial Q(s_t,a_t;\\omega)}{\\partial \\omega}$\n",
    "# Multi-step\n",
    "- Multi-step return\n",
    "  - $U_t = R_t + \\gamma\\cdot U_{t+1}$\n",
    "  - $U_t = \\sum^{m-1}_{i=0}\\gamma^i\\cdot R_{t+i}+\\gamma^m\\cdot U_{t+m}$\n",
    "- Multi-step TD targets\n",
    "  - m-step TD target for Sarsa:<br>&emsp;$y_t = \\sum^{m-1}_{i=0}\\gamma^i\\cdot r_{t+i}+\\gamma^m\\cdot Q_\\pi(s_{t+m},a_{t+m})$\n",
    "  - m_step TD target for Q-learning:<br>&emsp;$y_t = \\sum^{m-1}_{i=0}\\gamma^i\\cdot r_{t+i}+\\gamma^m\\cdot \\max\\limits_a Q^\\ast(s_{t+m},a)$\n",
    "# Actor-critic\n",
    "## value-based methods(critic)\n",
    "  - Use neural net $q(s,a;\\omega)$ to approximate $Q_{\\pi}(s,a)$\n",
    "  - $\\omega$ is the trainable parameters of the neural net\n",
    "  - input: state s and action a\n",
    "  - approximate action-value (scalar)\n",
    "## Policy-based methods(actor)\n",
    "  - Use neural net $\\pi(a|s;\\theta)$ to approximate $\\pi(a|s)$\n",
    "  - $\\theta$ is the trainable parameters of the neural net\n",
    "  - input: state s\n",
    "  - output: probability distributions over the actions\n",
    "## Network training\n",
    "- observe state $s_t$\n",
    "- Randomly sample action $a_t$ according to $\\pi(\\cdot|s_t;\\theta_t)$\n",
    "- perform $a_t$ and observe new state $s_{t+1}$ and reward $r_t$\n",
    "- update value network q use TD\n",
    "  - compute $q(s_t,a_t;\\omega_t)$ and $q(s_{t+1},a_{t+1};\\omega_t)$\n",
    "  - TD target: $y_t = r_t +\\gamma\\cdot q(s_{t+1},a_{t+1};\\omega_t)$\n",
    "  - Loss: $L(\\omega)=\\frac{1}{2}[q(s_t,a_t;\\omega)-y_t]^2$\n",
    "  - Gradient descent: $\\omega_{t+1} = \\omega_t-\\alpha\\cdot \\frac{\\partial L(\\omega)}{\\partial \\omega}\\Bigr|_{\\substack{\\omega=\\omega_t}}$\n",
    "- update policy network $\\pi$ using policy gradient\n",
    "  - Definition: State-value function approximated using neural networks\n",
    "    - $V(s;\\theta,\\omega)=\\sum_a\\pi(a|s;\\theta)\\cdot q(s,a;\\omega)$\n",
    "  - policy gradient: derivative of $V(s_t;\\theta,\\omega)$ w.r.t. $\\theta$. \n",
    "    - let $g(a,\\theta) = \\frac{\\partial log~{\\pi(a|s,\\theta)}}{\\partial \\theta}\\cdot q(s_t,a;\\omega)$\n",
    "    - $\\frac{\\partial V(s_t;\\theta,\\omega_t)}{\\partial \\theta} = \\mathbb{E}_A{[g(A,\\theta)]}$\n",
    "  - random sampling: $a \\sim \\pi(\\cdot|s_t;\\theta_t)$\n",
    "  - stochastic gradient ascent: $\\theta_{t+1}=\\theta_t+\\beta\\cdot g(a,\\theta_t)$\n",
    "- summary of Algorithm\n",
    "  1. Observe state $s_t$ and randomly sample $a_t\\sim \\pi(\\cdot|s_t;\\theta_t)$.\n",
    "  2.  Perform $a_t$ and observe new state $s_{t+1}$ and reward $r_t$\n",
    "  3. Randomly sample $\\tilde{a}_{t+1} \\sim \\pi(\\cdot|s_{t+1};\\theta_t)$\n",
    "  4.  Evaluate value network: $q(s_t,a_t;\\omega_t)$ and $q(s_{t+1},a_{t+1};\\omega_t)$\n",
    "  5.  Compute TD error: $\\delta_t = q_t - (r_t+\\gamma\\cdot q_{t+1})$\n",
    "  6.  Differentiate value network: $d_{w,t}=\\frac{\\partial q(s_t,a_t;\\omega)}{\\partial \\omega}\\bigr|_{\\substack{\\omega=\\omega_t}}$\n",
    "  7.  Update value network: $\\omega_{t+1} = \\omega_t -\\alpha\\cdot \\delta_t \\cdot d_{w,t}$\n",
    "  8.  Differentiate policy network: $d_{\\theta,t}= \\frac{\\partial log~{\\pi(a_t|s_t,\\theta)}}{\\partial \\theta} \\bigr|_{\\substack \\theta = \\theta_t}$\n",
    "  9.  Update policy network: $\\theta_{t+1}=\\theta_t+\\beta\\cdot \\delta_t \\cdot d_{\\theta,t}$\n",
    "# Paper\n",
    "confounded components (c-component), assign two variables to the same group iff they are connected by a path composed solely of bi-directional arrows."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
